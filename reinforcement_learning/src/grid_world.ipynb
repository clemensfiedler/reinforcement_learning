{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from windy_gridworld_env import WindyGridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WindyGridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        strategy=\"random\",\n",
    "        learning_rate=\"decaying-epsilon\",\n",
    "        learning_method=\"sarsa\",\n",
    "    ):\n",
    "        self.strategy = strategy\n",
    "        self.value_function = defaultdict(lambda: {0: 0, 1: 0, 2: 0, 3: 0})\n",
    "        self.t = 0\n",
    "        self.alpha = 0.01\n",
    "\n",
    "        self.learning_method = learning_method\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = 0.1\n",
    "\n",
    "    def _get_epsilon(self):\n",
    "        if self.learning_rate == \"decaying-epsilon\":\n",
    "            return 1 / (self.t + 1)\n",
    "        else:\n",
    "            return self.epsilon\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if self.strategy == \"random\":\n",
    "            return np.random.choice(4)\n",
    "        if self.strategy == \"epsilon-greedy\":\n",
    "            epsilon = self._get_epsilon()\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(4)\n",
    "            else:\n",
    "                action = np.argmax([self.value_function[state][i] for i in range(4)])\n",
    "            return int(action)\n",
    "\n",
    "    def update(self, state, action, reward, next_state, next_action):\n",
    "\n",
    "        if self.learning_method == \"sarsa\":\n",
    "            self.value_function[state][action] += self.alpha * (\n",
    "                reward\n",
    "                + self.value_function[next_state][next_action]\n",
    "                - self.value_function[state][action]\n",
    "            )\n",
    "        elif self.learning_method == \"q-learning\":\n",
    "            self.value_function[state][action] += self.alpha * (\n",
    "                reward\n",
    "                + max(self.value_function[next_state].values())\n",
    "                - self.value_function[state][action]\n",
    "            )\n",
    "\n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episodes(agent, n_episodes=2):\n",
    "    sequence = []\n",
    "    for r in range(n_episodes):\n",
    "\n",
    "        terminated = False\n",
    "\n",
    "        # initialize:\n",
    "        state = env.reset()\n",
    "        action = agent.get_action(state)\n",
    "\n",
    "        while not terminated:\n",
    "        # update loop\n",
    "            next_state, reward, terminated, _ = env.step(action)\n",
    "            next_action = agent.get_action(next_state)\n",
    "            agent.update(state, action, reward, next_state, next_action)\n",
    "            action, state = next_action, next_state\n",
    "\n",
    "            sequence.append((state[0], state[1], action, reward, r))\n",
    "\n",
    "    episodes = pl.DataFrame(\n",
    "        sequence, schema=[\"x\", \"y\", \"action\", \"reward\", \"episode\"], orient=\"row\"\n",
    "    )\n",
    "\n",
    "    return episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_function(agent):\n",
    "    shapes = {\n",
    "        0: \"↑\",\n",
    "        1: \"→\",\n",
    "        2: \"↓\",\n",
    "        3: \"←\",\n",
    "        -1: \"\",\n",
    "    }\n",
    "\n",
    "    data_plotting = pl.DataFrame(\n",
    "        [\n",
    "            (x, y, int(np.argmax([qs[i] for i in range(4)])), max(qs.values()))\n",
    "            for (x, y), qs in agent.value_function.items()\n",
    "        ],\n",
    "        orient=\"row\",\n",
    "        schema=[\"x\", \"y\", \"action\", \"value\"],\n",
    "    ).sort(\"x\", \"y\")\n",
    "\n",
    "    z = data_plotting.pivot(index=\"x\", on=\"y\", values=\"value\").fill_null(0)\n",
    "    actions = (\n",
    "        data_plotting.pivot(index=\"x\", on=\"y\", values=\"action\").drop(\"x\").fill_null(-1)\n",
    "    )\n",
    "\n",
    "    y = np.array(z[\"x\"])[::-1]\n",
    "    x = np.array([int(i) for i in z.columns[1:]])\n",
    "\n",
    "    z = z.drop(\"x\").to_numpy()\n",
    "\n",
    "\n",
    "    text_actions = np.empty(actions.shape, dtype=np.dtype(\"U3\"))\n",
    "    for yi in range(actions.shape[0]):\n",
    "        for xi in range(actions.shape[1]):\n",
    "            if (yi, xi) == (3, 0):\n",
    "                text_actions[yi, xi] = \"S \" + shapes[actions[yi, xi]]\n",
    "            elif (yi, xi) == (3, 7):\n",
    "                text_actions[yi, xi] = \"G\"\n",
    "            else:\n",
    "                text_actions[yi, xi] = shapes[actions[yi, xi]]\n",
    "\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(\n",
    "            z=z,\n",
    "            x=x,\n",
    "            y=y,\n",
    "            colorscale=\"Viridis\",\n",
    "            text=text_actions,\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={\"size\": 20},\n",
    "        )\n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = GridWorldAgent(strategy=\"epsilon-greedy\", learning_method=\"sarsa\")\n",
    "generate_episodes(agent, n_episodes=100_000)\n",
    "plot_value_function(agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = GridWorldAgent(strategy=\"epsilon-greedy\", learning_method=\"q-learning\")\n",
    "generate_episodes(agent, n_episodes=100_000)\n",
    "plot_value_function(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement-learning-PbRn6gH6-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
