{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Examples related to Chapter 6\n",
    "Sutton and Barto's Reinforcement Learning: An Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6.2 Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_episode(end_node=3):\n",
    "    location = 0\n",
    "\n",
    "    episode = [(location, 0)]\n",
    "\n",
    "    for i in range(100):\n",
    "        move = np.random.choice([-1, 1])\n",
    "        location += move\n",
    "\n",
    "\n",
    "        if np.abs(location) == end_node:\n",
    "            if location > 0:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "            episode.append((location, reward))\n",
    "            break\n",
    "        else:\n",
    "            episode.append((location, 0))\n",
    "\n",
    "    return np.array(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = [generate_episode() for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_polars_td(V_history):\n",
    "    iteration = []\n",
    "    state = []\n",
    "    value = []\n",
    "\n",
    "    for r, V in V_history.items():\n",
    "        for s, v in V.items():\n",
    "            iteration.append(r)\n",
    "            state.append(s)\n",
    "            value.append(v)\n",
    "\n",
    "    v_hist_df = pl.DataFrame(\n",
    "        {\"iteration\": iteration, \"state\": np.array(state), \"value\": np.array(value)}\n",
    "    )\n",
    "\n",
    "    return v_hist_df\n",
    "\n",
    "\n",
    "def temporal_difference(episodes):\n",
    "\n",
    "    alpha = 0.01\n",
    "    gamma = 1\n",
    "\n",
    "    V_history = {}\n",
    "    V = {k: 0 for k in np.arange(-3, 3 + 1, 1)}\n",
    "\n",
    "    for r, sequence in enumerate(episodes):\n",
    "        state = 0\n",
    "\n",
    "        for next_state, reward in sequence[1:]:\n",
    "            V[state] = V[state] + alpha * (reward + gamma * V[next_state] - V[state])\n",
    "            state = next_state\n",
    "\n",
    "        V_history[r] = dict(V.copy())\n",
    "\n",
    "    return V_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc_prediction(episodes):\n",
    "    gamma = 1\n",
    "\n",
    "    V_history = {}\n",
    "\n",
    "    values_by_state = defaultdict(list)\n",
    "\n",
    "    for r, sequence in enumerate(episodes):\n",
    "        G = 0\n",
    "        states_visited = []\n",
    "\n",
    "        for step in sequence[::-1]:\n",
    "            state, reward = step\n",
    "            G = gamma * G + reward\n",
    "            if state not in states_visited:\n",
    "                values_by_state[state].append(G)\n",
    "\n",
    "        V = {}\n",
    "        for state, values in values_by_state.items():\n",
    "            V[state] = np.mean(values)\n",
    "\n",
    "        V_history[r] = V\n",
    "\n",
    "    return V_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_history_mc = first_visit_mc_prediction(episodes)\n",
    "value_history_td = temporal_difference(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_combined = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            convert_to_polars_td(value_history_mc).with_columns(\n",
    "                pl.lit(\"td\").alias(\"method\")\n",
    "            ),\n",
    "            convert_to_polars_td(value_history_td).with_columns(\n",
    "                pl.lit(\"mc\").alias(\"method\")\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    .with_columns((((pl.col(\"state\") + 3) / 6) % 1).alias(\"value_true\"))\n",
    "    .with_columns((pl.col(\"value\") - pl.col(\"value_true\")).alias(\"error\"))\n",
    ")\n",
    "\n",
    "results_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    (\n",
    "        results_combined.filter((pl.col(\"iteration\") % 10) == 0).sort(\n",
    "            \"iteration\", \"state\"\n",
    "        )\n",
    "    ),\n",
    "    x=\"state\",\n",
    "    y=\"value\",\n",
    "    color=\"method\",\n",
    "    animation_frame=\"iteration\",\n",
    ")\n",
    "fig.update_yaxes(range=[0, 1])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement-learning-PbRn6gH6-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
